{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_mnist import MNIST\n",
    "import helper\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from cvxopt import matrix, solvers\n",
    "from itertools import product\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {4:-1, 9:1}\n",
    "X_train, Y_train = helper.subsetData(data.train_data, data.train_labels, label_dict)\n",
    "X_train, Y_train, _ = helper.shuffleArraysInUnison(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = helper.subsetData(data.test_data, data.test_labels, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBinaryClassifier(object):\n",
    "    \"\"\"\n",
    "    Class for Linear Binary Classifiers\n",
    "    \n",
    "    weights: np array of shape (dim, 1)\n",
    "    bias: scalar\n",
    "    \"\"\"\n",
    "    def __init__(self, weights, bias):\n",
    "        self.dim = weights.shape[0]\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: np array of shape (num_points, dim) \n",
    "        \n",
    "        returns: a vector of shape (num_points,) with predicted labels for each point\n",
    "        \"\"\"\n",
    "        return np.sign(np.matmul(X, self.weights) + self.bias).T[0]\n",
    "    \n",
    "    def distance(self, X):\n",
    "        \"\"\"\n",
    "        Computes the signed distance from a point to the decision boundary (hyperplane)\n",
    "        \n",
    "        returns: a vector of shape (num_points,) with the correspoding distances\n",
    "        \"\"\"\n",
    "        return abs((np.matmul(X, self.weights) + self.bias) / np.linalg.norm(self.weights)).T[0]\n",
    "    \n",
    "    def evaluate(self, X, Y):\n",
    "        \"\"\"\n",
    "        returns accuracy\n",
    "        \"\"\"\n",
    "        return np.mean(np.equal(self.predict(X), Y))\n",
    "    \n",
    "    def gradient(self, X, Y):\n",
    "        \"\"\"\n",
    "        returns gradient\n",
    "        \"\"\"\n",
    "        if not hasattr(Y, \"__len__\"): # make it robust to single items\n",
    "            X = X.reshape(1, self.dim)\n",
    "            Y = np.array([Y])\n",
    "            \n",
    "        return np.array([Y[i] * self.weights.reshape(-1,) if self.predict(X[i]) == Y[i] \n",
    "                         else np.zeros(self.dim) for i in xrange(len(X))])\n",
    "    \n",
    "    def rhinge_loss(self, X, Y):\n",
    "        if not hasattr(Y, \"__len__\"): # make it robust to single items\n",
    "            X = X.reshape(1, self.dim)\n",
    "            Y = np.array([Y])\n",
    "            \n",
    "        res = np.maximum(0, Y.reshape(-1,1) * (np.matmul(X, self.weights) + self.bias))\n",
    "        return np.mean(res.reshape(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainLBC(X, Y):\n",
    "    model = svm.SVC(kernel = \"linear\")\n",
    "    model.fit(X, Y)\n",
    "    return LinearBinaryClassifier(model.coef_.T, model.intercept_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 2690 0.96082370668\n",
      "1 2690 5380 0.960321446509\n",
      "2 5380 8070 0.960321446509\n",
      "3 8070 10760 0.957810145655\n"
     ]
    }
   ],
   "source": [
    "# train the classifiers\n",
    "n = 4\n",
    "train_size = len(X_train) / n\n",
    "\n",
    "binary_classifiers = []\n",
    "\n",
    "for i in xrange(n):\n",
    "    model = svm.SVC(kernel = \"linear\")\n",
    "    start = train_size * i\n",
    "    end = start + train_size\n",
    "    lbc = trainLBC(X_train[start:end], Y_train[start:end])\n",
    "    print i, start, end, lbc.evaluate(X_test, Y_test)\n",
    "    binary_classifiers.append(lbc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tryRegionBinary(models, signs, x, delta=1e-10):\n",
    "    \"\"\"\n",
    "    models: list of LinearBinaryClassifiers\n",
    "    signs: list of signs of length num_models\n",
    "    x: np array of shape dim\n",
    "    finds a vector in the region denoted by the signs vector\n",
    "    \"\"\"\n",
    "    dim = x.shape[0]\n",
    "    P = matrix(np.identity(dim))\n",
    "    q = matrix(np.zeros(dim))\n",
    "    h = []\n",
    "    G = []\n",
    "    num_models = len(models)\n",
    "    for i in xrange(num_models):\n",
    "        weights, bias = models[i].weights.T, models[i].bias\n",
    "        ineq_val  = -1.0 * delta + signs[i] * (np.dot(weights, x) + bias)\n",
    "        h.append(ineq_val[0])\n",
    "        G.append(-1.0 * signs[i] * weights.reshape(-1,))\n",
    "    h = matrix(h)\n",
    "    G = matrix(np.array(G))\n",
    "    solvers.options['show_progress'] = False\n",
    "    sol = solvers.qp(P, q, G, h)\n",
    "    if sol['status'] == 'optimal':\n",
    "        v = np.array(sol['x']).reshape(-1,)\n",
    "        perturbed_x = np.array(x + v).reshape(1, -1)\n",
    "        is_desired_sign = [models[i].predict(perturbed_x)[0] == signs[i] for i in xrange(num_models)]\n",
    "        if sum(is_desired_sign) == num_models:\n",
    "            return v\n",
    "        else:\n",
    "            return tryRegionBinary(models, signs, x, delta * 1.5)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "t = binary_classifiers[0]\n",
    "x = X_test[0]\n",
    "y = Y_test[0]\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0, -1.0, -1.0, -1.0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model.predict(x) for model in binary_classifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1.0, -1.0, -1.0, -1.0) [-1.0, -1.0, -1.0, -1.0]\n",
      "(-1.0, -1.0, -1.0, 1.0) [-1.0, -1.0, -1.0, 1.0]\n",
      "(-1.0, -1.0, 1.0, -1.0) [-1.0, -1.0, 1.0, -1.0]\n",
      "(-1.0, -1.0, 1.0, 1.0) [-1.0, -1.0, 1.0, 1.0]\n",
      "(-1.0, 1.0, -1.0, -1.0) [-1.0, 1.0, -1.0, -1.0]\n",
      "(-1.0, 1.0, -1.0, 1.0) [-1.0, 1.0, -1.0, 1.0]\n",
      "(-1.0, 1.0, 1.0, -1.0) [-1.0, 1.0, 1.0, -1.0]\n",
      "(-1.0, 1.0, 1.0, 1.0) [-1.0, 1.0, 1.0, 1.0]\n",
      "(1.0, -1.0, -1.0, -1.0) [1.0, -1.0, -1.0, -1.0]\n",
      "(1.0, -1.0, -1.0, 1.0) [1.0, -1.0, -1.0, 1.0]\n",
      "(1.0, -1.0, 1.0, -1.0) [1.0, -1.0, 1.0, -1.0]\n",
      "(1.0, -1.0, 1.0, 1.0) [1.0, -1.0, 1.0, 1.0]\n",
      "(1.0, 1.0, -1.0, -1.0) [1.0, 1.0, -1.0, -1.0]\n",
      "(1.0, 1.0, -1.0, 1.0) [1.0, 1.0, -1.0, 1.0]\n",
      "(1.0, 1.0, 1.0, -1.0) [1.0, 1.0, 1.0, -1.0]\n",
      "(1.0, 1.0, 1.0, 1.0) [1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "for s in product([-1.0, 1.0], repeat=n):\n",
    "    v = tryRegionBinary(binary_classifiers, s, x)\n",
    "    print s, [model.predict(x +  v) for model in binary_classifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distributionalOracle(distribution, models, x, y, alpha):\n",
    "    candidates = []\n",
    "    num_models = len(models)\n",
    "    # we should only take into consideration models that we could feasibly trick\n",
    "    dists = [model.distance(x) for model in models]\n",
    "    feasible_models = [models[i] for i in xrange(num_models) if dists[i] < alpha]\n",
    "    distribution = np.array([distribution[i] for i in xrange(num_models) if dists[i] < alpha])\n",
    "    num_models = len(feasible_models)\n",
    "    \n",
    "    # can't trick anything\n",
    "    if num_models == 0:\n",
    "        return np.zeros(x.shape)\n",
    "\n",
    "    signs_values = []\n",
    "    for signs in product([-1.0, 1.0], repeat=num_models):  # iterate over all possible regions\n",
    "        is_misclassified = np.equal(-1.0 * y * np.ones(num_models), signs)  # y = -1, or 1\n",
    "        value = np.dot(is_misclassified, distribution)\n",
    "        signs_values.append((signs, value))\n",
    "\n",
    "    values = sorted(set([value for signs, value in signs_values]), reverse=True)\n",
    "    for value in values:\n",
    "        feasible_candidates = []\n",
    "        for signs in [sign for sign, val in signs_values if val == value]:\n",
    "            v = tryRegionBinary(feasible_models, signs, x)\n",
    "            if v is not None:\n",
    "                norm = np.linalg.norm(v)\n",
    "                if norm <= alpha:\n",
    "                    feasible_candidates.append((v, norm))\n",
    "        # amongst those with the max value, return the one with the minimum norm\n",
    "        if feasible_candidates:\n",
    "            # break out of the loop since we have already found the optimal answer\n",
    "            return min(feasible_candidates, key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinateAscent(distribution, models, x, y, alpha, greedy=True):\n",
    "    \n",
    "    dists = [model.distance(x) for model in models]\n",
    "    num_models = len(models)\n",
    "    feasible_models = [models[i] for i in xrange(num_models) if dists[i] < alpha]\n",
    "    distribution = np.array([distribution[i] for i in xrange(num_models) if dists[i] < alpha])\n",
    "    num_models = len(feasible_models)\n",
    "    \n",
    "    sol = np.zeros(x.shape)\n",
    "    \n",
    "    # can't trick anything\n",
    "    if num_models == 0:\n",
    "        return sol\n",
    "    \n",
    "    signs = [y] * num_models # initialize to the original point, of length feasible_models\n",
    "    options = dict(zip(range(num_models), distribution))\n",
    "    for i in xrange(num_models):\n",
    "        \n",
    "        if greedy:\n",
    "            coord = max(options, key=options.get)\n",
    "        else:\n",
    "            coord = np.random.choice(options.keys())\n",
    "        \n",
    "        del options[coord]    \n",
    "        signs[coord] *= -1    \n",
    "        print signs\n",
    "        v = tryRegionBinary(feasible_models, signs, x)\n",
    "        \n",
    "        valid_sol = False\n",
    "        if v is not None:\n",
    "            norm = np.linalg.norm(v)\n",
    "            if norm <= alpha:\n",
    "                valid_sol = True\n",
    "                sol = v\n",
    "        if not valid_sol:\n",
    "            break\n",
    "        \n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(distribution, models, x, y, alpha):\n",
    "    learning_rate = .001\n",
    "    T = 1000\n",
    "    v = np.zeros(len(x))\n",
    "    for i in xrange(T):\n",
    "        loss = np.dot(distribution, [model.rhinge_loss(x + v, y) for model in models])\n",
    "        if loss == 0:\n",
    "            break\n",
    "        print i, loss, np.mean([model.predict(x + v) for model in models])\n",
    "        gradient = sum([-1 * w * model.gradient(x + v, y) for w, model in zip(distribution, models)])[0]\n",
    "        v += learning_rate * gradient\n",
    "        norm  = np.linalg.norm(v)\n",
    "        if norm >= alpha:\n",
    "            v = v / norm * alpha\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 1, -1, -1]\n",
      "[-1, 1, -1, 1]\n",
      "[-1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "0.436450081725\n"
     ]
    }
   ],
   "source": [
    "v = coordinateAscent([1] * n, binary_classifiers, X_test[0], Y_test[0], .55, greedy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model.rhinge_loss(X_test[0] + v, -1) for model in binary_classifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "greedyCoordinateAscent = partial(coordinateAscent, greedy=True)\n",
    "randomCoordinateAscent = partial(coordinateAscent, greedy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 1, -1, -1]\n",
      "[1, 1, -1, -1]\n",
      "[1, 1, 1, -1]\n",
      "[1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "        -1.26602820e-03,  -3.93703878e-03,  -2.45485956e-03,\n",
       "         1.04708501e-16,  -1.58642993e-05,  -1.07965515e-03,\n",
       "        -2.30786632e-03,  -1.27231266e-03,  -9.87459238e-05,\n",
       "        -6.87029122e-06,   5.35464509e-09,   1.04708501e-16,\n",
       "        -7.69051939e-04,  -8.91772983e-04,  -3.84525938e-04,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "        -1.62044744e-09,  -3.53174337e-10,  -1.48886649e-04,\n",
       "        -4.21967631e-04,  -3.27203620e-03,  -2.96926806e-03,\n",
       "        -3.48282581e-03,  -7.57496402e-03,  -1.49990296e-02,\n",
       "        -1.70369946e-02,  -4.85150403e-03,  -1.13690520e-02,\n",
       "        -1.23910409e-02,  -3.42600183e-02,  -2.87847417e-02,\n",
       "        -7.12804962e-03,  -4.13605554e-03,  -3.42425444e-03,\n",
       "        -8.83591580e-04,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,  -9.07173567e-10,  -1.64580961e-04,\n",
       "        -8.88484569e-04,  -5.08489054e-03,  -9.16239908e-03,\n",
       "         1.50102855e-02,  -8.77444917e-03,   6.29270782e-03,\n",
       "         3.28831202e-02,   1.60030090e-02,   2.25135796e-03,\n",
       "        -1.59544953e-02,   9.73093950e-03,  -2.28637908e-04,\n",
       "         1.37375215e-02,   1.67803973e-02,   7.33317835e-03,\n",
       "        -2.04268600e-02,  -2.15013832e-02,  -4.62726706e-03,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   8.10064543e-04,  -7.75178310e-03,\n",
       "        -1.16687246e-02,  -1.95993724e-02,  -7.96452668e-03,\n",
       "        -2.21425296e-03,   6.18664657e-03,   2.82177017e-02,\n",
       "         2.02167190e-02,   2.58765319e-02,   3.05100668e-02,\n",
       "         2.49555770e-02,   3.11590167e-02,   3.68709719e-02,\n",
       "         3.02064475e-02,   5.59542237e-03,   3.72342499e-02,\n",
       "        -8.70440058e-03,  -3.08251845e-02,  -2.50151492e-02,\n",
       "        -1.54160046e-02,  -8.70071356e-04,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,  -1.43938655e-03,\n",
       "        -2.30048440e-02,  -9.30655915e-03,  -1.98260369e-02,\n",
       "         1.74033676e-02,   1.17271028e-02,   2.23736605e-02,\n",
       "         1.46841437e-02,   4.88544659e-02,   1.70982111e-02,\n",
       "         5.04678881e-02,   3.11852501e-02,   6.65945387e-02,\n",
       "        -3.40356254e-03,   2.31518088e-02,   3.44973137e-03,\n",
       "         1.53539541e-02,   4.04577302e-03,  -3.22768731e-03,\n",
       "        -2.43280441e-02,  -2.09745803e-02,  -2.90316626e-03,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.20345975e-03,\n",
       "         1.98821891e-03,   2.85265145e-03,   5.63056260e-04,\n",
       "        -1.31892243e-02,   2.04735677e-02,  -1.30259876e-02,\n",
       "         1.53088338e-02,  -4.38507586e-03,   3.61183865e-02,\n",
       "         5.40207140e-02,   1.63478008e-02,   2.10675479e-02,\n",
       "         5.14341160e-02,   1.89244003e-02,   4.05863014e-03,\n",
       "         2.56457157e-02,  -6.56268548e-03,   1.22205978e-02,\n",
       "         3.51111939e-03,  -1.05342296e-02,   2.44480225e-03,\n",
       "         4.90307917e-03,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         4.24658122e-03,   9.53302486e-03,  -2.51265362e-03,\n",
       "         2.22263605e-02,   1.14716820e-02,  -1.83434752e-02,\n",
       "         1.73535582e-02,   2.43771610e-02,  -6.04526495e-03,\n",
       "         5.18958504e-03,   2.10845068e-02,   2.79958379e-02,\n",
       "         4.54521594e-02,  -9.80971716e-03,   4.65860521e-03,\n",
       "         1.05637444e-02,   1.87306009e-03,  -3.91056833e-03,\n",
       "        -6.64023747e-03,   2.75745217e-02,  -1.38191103e-02,\n",
       "         2.19818659e-02,   1.39974322e-03,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   5.05577797e-03,  -1.37081958e-03,\n",
       "         2.02978788e-04,   1.11209825e-02,  -1.70065187e-03,\n",
       "         7.06677984e-04,   8.72990480e-04,  -6.39556405e-03,\n",
       "        -9.49308814e-03,  -2.92081462e-02,   4.36305086e-02,\n",
       "         3.29146226e-02,  -5.04854658e-03,   2.61153293e-02,\n",
       "        -2.58051554e-02,  -5.19277681e-03,  -2.42439487e-02,\n",
       "         1.26209307e-03,  -2.93416141e-03,   8.52132989e-03,\n",
       "         1.61320638e-02,  -1.84873418e-03,   2.21046676e-03,\n",
       "         4.11288360e-09,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.05109699e-08,  -3.27599309e-03,\n",
       "         1.32759921e-02,  -2.88126336e-03,   7.77265146e-03,\n",
       "         1.99256960e-02,  -1.27819769e-03,   2.89780526e-03,\n",
       "         4.12633316e-03,   2.81489537e-02,  -1.30482895e-02,\n",
       "         3.65809964e-02,  -1.55257426e-02,  -2.93005485e-02,\n",
       "        -1.17869572e-02,   9.92496980e-03,   1.11059535e-02,\n",
       "         1.31165123e-02,   1.54199048e-02,   8.84377951e-03,\n",
       "         2.61846848e-02,   2.36438406e-02,   9.24776155e-03,\n",
       "         3.59055460e-03,   9.22283071e-09,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   2.57816251e-09,\n",
       "        -4.94164575e-03,   3.16612758e-02,  -1.31503734e-02,\n",
       "         2.60577859e-03,   2.26263939e-03,  -2.38730573e-02,\n",
       "         1.47388490e-02,  -4.32356547e-02,  -1.79579615e-02,\n",
       "        -6.77098531e-02,  -2.81931093e-02,   4.36321613e-02,\n",
       "         1.43967258e-02,   1.45619020e-02,   2.53177302e-02,\n",
       "         2.21368397e-02,   2.98363310e-02,   1.90441284e-02,\n",
       "         3.18448884e-04,  -3.68610688e-03,   6.34927840e-03,\n",
       "        -8.99771517e-03,  -2.12326136e-03,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,  -6.79040492e-03,   1.69726095e-02,\n",
       "        -1.15052837e-02,  -1.35906463e-02,   2.50089379e-02,\n",
       "        -2.53856704e-02,  -1.38540708e-02,   1.03027451e-03,\n",
       "        -5.09017529e-02,  -1.97493877e-02,  -2.89789981e-03,\n",
       "         1.36442972e-02,  -2.10669395e-02,  -1.28555834e-02,\n",
       "        -4.57857079e-03,   5.70716397e-03,  -7.20045300e-03,\n",
       "        -1.58969690e-02,   8.28125632e-03,  -2.05240807e-02,\n",
       "        -3.33074337e-02,  -9.50799632e-03,  -1.02068465e-03,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,  -5.05365154e-05,  -9.68697182e-03,\n",
       "         1.05442061e-02,  -2.27161547e-02,  -2.86640221e-02,\n",
       "        -2.11788303e-02,  -1.73285383e-02,  -2.10326969e-02,\n",
       "        -1.77373689e-02,  -1.26942452e-02,  -1.50955972e-02,\n",
       "         1.41720535e-02,  -5.91125494e-03,   1.98688244e-02,\n",
       "         6.72404164e-03,  -2.11179891e-02,   2.24449170e-03,\n",
       "        -2.67278182e-02,   4.88967503e-03,   1.72419386e-02,\n",
       "        -2.21244218e-02,  -1.99482252e-02,  -8.26422121e-03,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,  -1.92038909e-03,\n",
       "        -1.23449067e-02,   7.38639179e-03,  -9.66467153e-03,\n",
       "        -2.55106951e-02,  -9.30045272e-03,  -6.93180470e-03,\n",
       "        -2.45664881e-02,  -3.74736439e-03,  -6.69102854e-03,\n",
       "        -2.61908620e-02,  -3.38760126e-03,  -2.80549626e-02,\n",
       "        -6.49045871e-03,  -6.74516933e-03,  -9.16840172e-03,\n",
       "        -7.08198820e-03,  -7.14331316e-03,   1.10000629e-02,\n",
       "        -7.18646239e-03,  -8.50157657e-03,  -1.07186830e-02,\n",
       "        -1.75961141e-03,  -2.63791989e-03,  -2.63791989e-03,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "        -5.65124405e-11,  -6.82454855e-03,  -2.37767498e-04,\n",
       "        -2.95129999e-03,   2.40328432e-03,  -5.10008520e-03,\n",
       "        -1.26616260e-02,  -1.64615161e-02,   7.28851428e-03,\n",
       "         3.45269628e-02,  -5.85998871e-03,  -3.35245498e-02,\n",
       "        -1.21931886e-02,  -1.85560825e-02,  -1.26451511e-02,\n",
       "         8.43277447e-03,  -2.18334264e-02,  -1.24474590e-02,\n",
       "        -1.63619157e-02,  -1.20889426e-02,  -1.98761410e-03,\n",
       "         2.43032597e-03,  -1.75543430e-03,  -2.63791989e-03,\n",
       "        -4.27091762e-04,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,  -2.06495179e-03,\n",
       "        -1.24267919e-02,  -1.67973600e-02,  -1.41931629e-03,\n",
       "        -1.96380283e-02,  -1.10464247e-02,   6.41930744e-03,\n",
       "         1.45138285e-02,  -1.00785826e-02,  -3.94620821e-03,\n",
       "        -2.46951142e-03,  -3.11418074e-02,  -3.01973416e-02,\n",
       "        -1.96101753e-02,   2.95254786e-03,  -2.56287223e-02,\n",
       "         2.61421870e-03,  -1.35585663e-02,  -3.65712046e-02,\n",
       "        -1.45725728e-02,  -9.92360989e-03,  -8.04266126e-03,\n",
       "        -6.21174698e-05,   1.04708501e-16,   1.04708501e-16,\n",
       "         5.81195203e-03,   1.04708501e-16,   1.04708501e-16,\n",
       "        -3.22105771e-03,  -8.73852047e-03,  -2.68522921e-02,\n",
       "        -5.18041225e-03,  -1.03045358e-02,  -1.12088987e-02,\n",
       "         1.52452306e-02,   4.35788963e-02,  -7.37837789e-03,\n",
       "        -4.53209747e-02,  -1.80631756e-02,   4.04105962e-03,\n",
       "        -9.32404109e-03,  -3.52217283e-02,   4.09524239e-03,\n",
       "        -8.04877793e-03,   1.64858562e-02,   1.18653345e-02,\n",
       "        -1.26096817e-02,   6.52952832e-03,  -1.05778636e-02,\n",
       "        -1.13673402e-02,   4.74123977e-03,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,  -4.85437218e-03,  -1.69615793e-03,\n",
       "        -1.17816848e-02,  -3.14973099e-03,  -7.01244680e-03,\n",
       "        -5.03952840e-03,   1.89966566e-02,   2.50282205e-02,\n",
       "        -2.90498951e-02,  -1.95795039e-04,  -2.96939508e-03,\n",
       "        -2.68840962e-02,   4.59141805e-03,  -1.37511508e-02,\n",
       "        -2.94147422e-02,   1.41210852e-02,   1.66693920e-02,\n",
       "         1.70771037e-03,  -2.24847540e-03,   1.69571845e-02,\n",
       "         6.16236685e-03,   2.67622574e-03,   5.77461764e-03,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   8.19283850e-06,\n",
       "         3.98171839e-03,   2.95176594e-03,  -6.94597486e-03,\n",
       "         2.38936152e-04,   1.05720077e-02,  -1.18917730e-02,\n",
       "        -2.22847044e-02,  -8.89094530e-03,  -3.34373731e-03,\n",
       "         9.32701027e-04,  -1.58645486e-02,  -6.61993522e-03,\n",
       "        -5.92595027e-04,   1.41820079e-02,   1.42325604e-03,\n",
       "         3.56313026e-02,   4.35936645e-03,   3.67955723e-03,\n",
       "         3.23005156e-02,   3.12796985e-02,   8.60148700e-03,\n",
       "         4.82407051e-03,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   2.45140229e-04,   8.44335015e-03,\n",
       "         7.95933325e-04,   2.70275948e-03,  -1.29563856e-02,\n",
       "         3.36650700e-03,   1.13146579e-02,  -5.32525143e-03,\n",
       "         6.82514331e-03,   9.66623363e-03,   1.99329742e-02,\n",
       "         5.25365049e-04,   1.11102751e-02,   9.61122233e-03,\n",
       "        -5.36632412e-03,   9.37922608e-03,   2.20089422e-02,\n",
       "         1.57472042e-02,   1.62507468e-02,   1.93643650e-02,\n",
       "         1.94098125e-02,   1.20766123e-03,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "        -2.11488384e-03,   6.58467810e-03,   7.66422766e-03,\n",
       "         4.04686388e-03,  -3.99467341e-03,  -6.21781567e-03,\n",
       "        -3.82646483e-03,   1.82504998e-02,  -1.39299684e-02,\n",
       "         1.17853993e-02,  -5.71175791e-03,   7.94935858e-03,\n",
       "         5.29782310e-03,  -4.05294646e-03,  -2.92823728e-02,\n",
       "         1.01796364e-03,   6.76770692e-04,   8.25892033e-03,\n",
       "         2.74934005e-02,   1.82704173e-02,   8.45362741e-03,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         7.54258529e-04,   5.14307768e-03,   2.32199328e-02,\n",
       "         4.03800148e-02,   8.35628329e-03,   1.03200014e-02,\n",
       "         5.35032649e-03,   9.78551780e-03,  -1.99465408e-02,\n",
       "         6.52616481e-03,   1.04560175e-02,   3.03077955e-03,\n",
       "        -4.65536102e-03,  -8.55050452e-03,  -1.02388829e-02,\n",
       "        -5.09959731e-03,  -2.13659399e-02,  -1.03016792e-02,\n",
       "         5.64225862e-03,   1.65752213e-02,   1.22181395e-02,\n",
       "         2.18088322e-03,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   5.55162532e-03,   1.18924789e-02,\n",
       "         2.39737241e-02,   2.22387809e-02,   2.80289676e-02,\n",
       "         2.03756334e-02,   4.44270410e-03,   2.42894801e-02,\n",
       "         2.09179701e-02,  -5.97969408e-03,  -2.16483199e-03,\n",
       "         3.00143480e-03,  -7.63450581e-03,   1.29547365e-02,\n",
       "         1.35522066e-02,  -7.33270132e-03,   2.87443361e-03,\n",
       "         5.73795063e-03,   3.11151233e-04,  -3.15068938e-03,\n",
       "         2.35630146e-03,  -1.21481875e-03,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,  -1.44737390e-03,\n",
       "         8.08538130e-03,   2.51217269e-02,   2.12478701e-02,\n",
       "         6.22619835e-02,   4.17583761e-02,   2.61771064e-03,\n",
       "         2.06534814e-02,   4.99829733e-02,   3.09878159e-02,\n",
       "         3.28296406e-02,   2.93656989e-02,   2.57012667e-02,\n",
       "         4.37830990e-02,   2.41742125e-02,   1.81794726e-02,\n",
       "         3.78957368e-02,   2.59167147e-02,   8.77888980e-03,\n",
       "         1.94455576e-03,   1.93809559e-03,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "        -3.92797206e-03,  -2.32177440e-03,   1.09234104e-02,\n",
       "         2.03525164e-02,   1.96200857e-02,   3.20418816e-02,\n",
       "         6.08026548e-02,   5.12464625e-02,   5.52029981e-02,\n",
       "         4.73505747e-02,   9.17078403e-02,   5.90426594e-02,\n",
       "         3.77616656e-02,   3.51928229e-02,   6.00624340e-02,\n",
       "         3.74447987e-02,   2.34252926e-02,   1.95460320e-02,\n",
       "         4.77450921e-03,   8.24707737e-04,   1.09969172e-04,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.83673372e-09,   1.04380235e-08,   1.30937044e-05,\n",
       "         4.18696469e-04,   7.08478632e-03,   4.97570152e-03,\n",
       "         2.76477191e-03,   2.56289302e-03,   1.75203436e-02,\n",
       "         7.90147764e-03,   1.04708501e-16,   4.28793579e-03,\n",
       "         1.41951692e-02,   7.21345119e-04,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16,   1.04708501e-16,   1.04708501e-16,\n",
       "         1.04708501e-16])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedyCoordinateAscent([3,4,2,1], binary_classifiers, X_test[0], Y_test[0], .55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adversary(distribution, models, X, Y, alpha, noiseFunc):\n",
    "    return np.array([noiseFunc(distribution, models, x, y, alpha) for x, y in zip(X,Y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateCosts(models, V, X, Y):\n",
    "    return np.array([1 - model.evaluate(X + V, Y) for model in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runMWU(models, T, X, Y, alpha, noiseFunc, epsilon=None):\n",
    "    num_models = len(models)\n",
    "\n",
    "    if epsilon is None:\n",
    "        delta = np.sqrt(4 * np.log(num_models) / float(T))\n",
    "        epsilon = delta / 2.0\n",
    "    else:\n",
    "        delta = 2.0 * epsilon\n",
    "\n",
    "    print \"Running MWU for {} Iterations with Epsilon {}\\n\".format(T, epsilon)\n",
    "\n",
    "    print \"Guaranteed to be within {} of the minimax value \\n\".format(delta)\n",
    "\n",
    "    loss_history = []\n",
    "    costs = []\n",
    "    max_acc_history = []\n",
    "    v = []\n",
    "    w = []\n",
    "\n",
    "    w.append(np.ones(num_models) / num_models)\n",
    "\n",
    "    for t in xrange(T):\n",
    "        print \"Iteration \", t\n",
    "        print\n",
    "        start_time = time.time()\n",
    "\n",
    "        v_t = adversary(w[t], models, X, Y, alpha, noiseFunc)\n",
    "        v.append(v_t)\n",
    "\n",
    "        cost_t = evaluateCosts(models, v_t, X, Y)\n",
    "        costs.append(cost_t)\n",
    "\n",
    "        print \"Shape of costs matrix\", np.array(costs).shape\n",
    "        avg_acc = np.mean((1 - np.array(costs)), axis=0)\n",
    "        max_acc = max(avg_acc)\n",
    "        max_acc_history.append(max_acc)\n",
    "\n",
    "        loss = np.dot(w[t], cost_t)\n",
    "        individual = [w[t][j] * cost_t[j] for j in xrange(num_models)]\n",
    "        \n",
    "        \n",
    "        print \"Weights, \", w[t], sum(w[t])\n",
    "        print \"Maximum (Average) Accuracy of Classifier \", max_acc\n",
    "        print \"Cost (Before Noise), \", np.array([1 - model.evaluate(X, Y) for model in models])\n",
    "        print \"Cost (After Noise), \", cost_t\n",
    "        print \"Loss, \", loss, individual\n",
    "\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        new_w = np.copy(w[t])\n",
    "\n",
    "        # penalize experts\n",
    "        for i in xrange(num_models):\n",
    "            new_w[i] *= (1.0 - epsilon) ** cost_t[i]\n",
    "\n",
    "        # renormalize weights\n",
    "        w_sum = new_w.sum()\n",
    "        for i in xrange(num_models - 1):\n",
    "            new_w[i] = new_w[i] / w_sum\n",
    "        new_w[-1] = 1.0 - new_w[:-1].sum()\n",
    "\n",
    "        w.append(new_w)\n",
    "\n",
    "        print\n",
    "        print \"time spent \", time.time() - start_time\n",
    "        print\n",
    "\n",
    "    return w, v, loss_history, max_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_experiment, Y_experiment = helper.generate_data(200, X_test, Y_test, binary_classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findNoiseBounds(models, X, Y):\n",
    "    max_bounds = []\n",
    "    num_models = len(models)\n",
    "    for i in xrange(len(X)):\n",
    "        max_r = -1 * Y[i] * np.ones(num_models)\n",
    "        max_v = tryRegionBinary(models, max_r, X[i])\n",
    "        max_bounds.append(np.linalg.norm(max_v))\n",
    "    min_bounds = np.array([model.distance(X) for model in models]).T\n",
    "    min_bounds = np.mean(min_bounds, axis=1)\n",
    "    return max_bounds, min_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm, mi = findNoiseBounds(binary_classifiers, X_experiment, Y_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.58697980263832394, 0.38746338441208694)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mm), np.mean(mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 200)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([mm, mi]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MWU for 20 Iterations with Epsilon 0.263276884773\n",
      "\n",
      "Guaranteed to be within 0.526553769547 of the minimax value \n",
      "\n",
      "Iteration  0\n",
      "\n",
      "Shape of costs matrix (1, 4)\n",
      "Weights,  [ 0.25  0.25  0.25  0.25] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.53\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.485  0.47   0.53   0.51 ]\n",
      "Loss,  0.49875 [0.12125, 0.11749999999999999, 0.13250000000000001, 0.1275]\n",
      "\n",
      "time spent  21.1433820724\n",
      "\n",
      "Iteration  1\n",
      "\n",
      "Shape of costs matrix (2, 4)\n",
      "Weights,  [ 0.25104631  0.25219953  0.24761818  0.24913598] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.5225\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.49   0.485  0.52   0.51 ]\n",
      "Loss,  0.50115026738 [0.12301269115235855, 0.12231677270255244, 0.12876145531583863, 0.12705934820912654]\n",
      "\n",
      "time spent  22.180134058\n",
      "\n",
      "Iteration  2\n",
      "\n",
      "Shape of costs matrix (3, 4)\n",
      "Weights,  [ 0.25190065  0.25344469  0.2461938   0.24846086] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.526666666667\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.48   0.465  0.54   0.485]\n",
      "Loss,  0.492212261836 [0.12091231187876247, 0.11785178225266991, 0.132944651455768, 0.12050351624830155]\n",
      "\n",
      "time spent  22.7672989368\n",
      "\n",
      "Iteration  3\n",
      "\n",
      "Shape of costs matrix (4, 4)\n",
      "Weights,  [ 0.25283292  0.25555123  0.24261615  0.24899971] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.52\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.465  0.505  0.535  0.505]\n",
      "Loss,  0.502165167593 [0.11756730783392967, 0.12905336953280966, 0.12979963839349609, 0.12574485183274289]\n",
      "\n",
      "time spent  28.4550189972\n",
      "\n",
      "Iteration  4\n",
      "\n",
      "Shape of costs matrix (5, 4)\n",
      "Weights,  [ 0.255713    0.25532264  0.24018738  0.24877698] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.522\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.47   0.485  0.52   0.505]\n",
      "Loss,  0.494546402948 [0.12018510942628635, 0.12383148052569164, 0.124897437947933, 0.12563237504766572]\n",
      "\n",
      "time spent  27.0304379463\n",
      "\n",
      "Iteration  5\n",
      "\n",
      "Shape of costs matrix (6, 4)\n",
      "Weights,  [ 0.25763372  0.25606416  0.23832264  0.24797948] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.52\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.49   0.475  0.535  0.495]\n",
      "Loss,  0.498123453871 [0.12624052361158589, 0.12163047498483623, 0.12750261288632178, 0.12274984238807886]\n",
      "\n",
      "time spent  24.8303029537\n",
      "\n",
      "Iteration  6\n",
      "\n",
      "Shape of costs matrix (7, 4)\n",
      "Weights,  [ 0.2582682   0.25787394  0.23564717  0.2482107 ] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.522142857143\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.465  0.495  0.52   0.505]\n",
      "Loss,  0.495625240165 [0.12009471229820404, 0.12764759996936337, 0.122536526603217, 0.12534640129443442]\n",
      "\n",
      "time spent  26.162596941\n",
      "\n",
      "Iteration  7\n",
      "\n",
      "Shape of costs matrix (8, 4)\n",
      "Weights,  [ 0.26069132  0.25791834  0.23389428  0.24749607] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.524375\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.46   0.51   0.515  0.505]\n",
      "Loss,  0.496897425082 [0.1199180069407705, 0.13153835206325498, 0.12045555262597925, 0.12498551345168581]\n",
      "\n",
      "time spent  24.4109139442\n",
      "\n",
      "Iteration  8\n",
      "\n",
      "Shape of costs matrix (9, 4)\n",
      "Weights,  [ 0.26364085  0.25688194  0.2325988   0.24687842] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.524444444444\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.475  0.465  0.535  0.49 ]\n",
      "Loss,  0.490090284686 [0.12522940331249346, 0.11945010056996834, 0.1244403560911673, 0.12097042471189055]\n",
      "\n",
      "time spent  24.6115589142\n",
      "\n",
      "Iteration  9\n",
      "\n",
      "Shape of costs matrix (10, 4)\n",
      "Weights,  [ 0.26485071  0.25885048  0.22942153  0.24687729] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.5245\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.475  0.475  0.51   0.5  ]\n",
      "Loss,  0.489201685558 [0.12580408837746818, 0.12295397601177369, 0.1170049784076439, 0.12343864276131222]\n",
      "\n",
      "time spent  23.5233211517\n",
      "\n",
      "Iteration  10\n",
      "\n",
      "Shape of costs matrix (11, 4)\n",
      "Weights,  [ 0.26599956  0.25997329  0.22796574  0.24606141] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.523181818182\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.49   0.47   0.515  0.515]\n",
      "Loss,  0.496651212886 [0.13033978290338574, 0.12218744776786554, 0.11740235590939309, 0.12672162630517322]\n",
      "\n",
      "time spent  22.4065420628\n",
      "\n",
      "Iteration  11\n",
      "\n",
      "Shape of costs matrix (12, 4)\n",
      "Weights,  [ 0.26653625  0.26209457  0.22668749  0.2446817 ] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.522916666667\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.48   0.455  0.5    0.5  ]\n",
      "Loss,  0.482875019617 [0.12793739881602395, 0.11925302715399098, 0.11334374564367021, 0.12234084800283485]\n",
      "\n",
      "time spent  22.8052270412\n",
      "\n",
      "Iteration  12\n",
      "\n",
      "Shape of costs matrix (13, 4)\n",
      "Weights,  [ 0.2667662   0.26433211  0.22550084  0.24340085] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.523076923077\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.475  0.47   0.525  0.485]\n",
      "Loss,  0.487387389886 [0.12671394417749246, 0.12423609336912951, 0.11838794063166264, 0.11804941170769745]\n",
      "\n",
      "time spent  25.5397150517\n",
      "\n",
      "Iteration  13\n",
      "\n",
      "Shape of costs matrix (14, 4)\n",
      "Weights,  [ 0.26777229  0.26573468  0.22291958  0.24357346] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.523571428571\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.47  0.47  0.52  0.52]\n",
      "Loss,  0.493324651622 [0.12585297515766214, 0.12489529959553756, 0.11591817905308718, 0.12665819781571311]\n",
      "\n",
      "time spent  25.5363099575\n",
      "\n",
      "Iteration  14\n",
      "\n",
      "Shape of costs matrix (15, 4)\n",
      "Weights,  [ 0.2696796   0.26762748  0.22110364  0.24158927] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.522333333333\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.495  0.49   0.53   0.505]\n",
      "Loss,  0.503816382747 [0.13349140375827484, 0.13113746632323139, 0.11718492956265142, 0.12200258310304322]\n",
      "\n",
      "time spent  27.8979561329\n",
      "\n",
      "Iteration  15\n",
      "\n",
      "Shape of costs matrix (16, 4)\n",
      "Weights,  [ 0.27040422  0.26875685  0.21933953  0.2414994 ] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.5221875\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.48   0.49   0.51   0.495]\n",
      "Loss,  0.492890245379 [0.12979402503137369, 0.13169085856812293, 0.11186315968748115, 0.11954220209167324]\n",
      "\n",
      "time spent  22.0404009819\n",
      "\n",
      "Iteration  16\n",
      "\n",
      "Shape of costs matrix (17, 4)\n",
      "Weights,  [ 0.27146989  0.26899289  0.21819472  0.24134251] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.521470588235\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.49   0.49   0.51   0.495]\n",
      "Loss,  0.495570606926 [0.13302024537002632, 0.1318065138387558, 0.11127930704804991, 0.11946454066922151]\n",
      "\n",
      "time spent  19.4234800339\n",
      "\n",
      "Iteration  17\n",
      "\n",
      "Shape of costs matrix (18, 4)\n",
      "Weights,  [ 0.27193155  0.26945033  0.21723423  0.24138389] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.520833333333\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.49   0.475  0.52   0.495]\n",
      "Loss,  0.493682191272 [0.13324645969560592, 0.12798890906840193, 0.11296179901122995, 0.11948502349684303]\n",
      "\n",
      "time spent  19.4511711597\n",
      "\n",
      "Iteration  18\n",
      "\n",
      "Shape of costs matrix (19, 4)\n",
      "Weights,  [ 0.27223451  0.27098967  0.21549191  0.24128392] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.520526315789\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.485  0.495  0.51   0.49 ]\n",
      "Loss,  0.494303613926 [0.13203373624411813, 0.13413988749179942, 0.10990087171617891, 0.11822911847432424]\n",
      "\n",
      "time spent  19.2989099026\n",
      "\n",
      "Iteration  19\n",
      "\n",
      "Shape of costs matrix (20, 4)\n",
      "Weights,  [ 0.27300844  0.27093099  0.21446008  0.24160048] 1.0\n",
      "Maximum (Average) Accuracy of Classifier  0.52\n",
      "Cost (Before Noise),  [ 0.  0.  0.  0.]\n",
      "Cost (After Noise),  [ 0.49  0.47  0.53  0.52]\n",
      "Loss,  0.500407798011 [0.13377413701095606, 0.12733756571690577, 0.113663844607163, 0.12563225067588299]\n",
      "\n",
      "time spent  19.3060679436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = runMWU(binary_classifiers, 20, X_experiment, Y_experiment, .5, gradientDescent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train0Layer(train_X, train_Y):\n",
    "    dim = train_X.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=dim, activation=\"linear\", kernel_regularizer=regularizers.l2(0.01),))\n",
    "    model.compile(loss=\"hinge\", optimizer=\"sgd\")\n",
    "    model.fit(train_X, train_Y, nb_epoch=50, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nns = []\n",
    "train_size = 1000\n",
    "init = 5\n",
    "for i in xrange(5):\n",
    "    start = train_size * (i + init)\n",
    "    end = start + train_size\n",
    "    print start, end\n",
    "    model = train0Layer(X_train[start:end], Y_train[start:end])\n",
    "    nns.append(model)\n",
    "    print model.evaluate(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train0Layer(X_train, Y_train, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
