import numpy as np
from cvxopt import matrix, solvers
from itertools import product
from functools import partial
import sys

GREEDY_DICT = dict(np.load("greedy_dict.npy"))


def tryRegionOneVsAll(models, labels, x, delta=1e-10):
    P = matrix(np.identity(x.shape[0]))
    q = matrix(np.zeros(x.shape[0]))
    h = []
    G = []
    num_models = len(models)
    for i in xrange(num_models):
        others = range(10)
        target = labels[i]
        del others[target]
        target_w, target_b = models[i].weights[target], models[i].bias[target]
        for j in others:
            other_w, other_b = models[i].weights[j], models[i].bias[j]
            ineq_val = np.dot(target_w - other_w, x) + target_b - other_b - delta
            h.append(ineq_val)
            G.append(other_w - target_w)
    h = matrix(h)
    G = matrix(np.array(G))
    solvers.options['show_progress'] = False
    sol = solvers.qp(P, q, G, h)
    if sol['status'] == 'optimal':
        v = np.array(sol['x']).reshape(-1,)
        perturbed_x = np.array(x + v).reshape(1, -1)
        is_desired_label = [models[i].predict(perturbed_x)[0] == labels[i] for i in xrange(num_models)]
        if sum(is_desired_label) == num_models:
            return v
        else:
            return tryRegionOneVsAll(models, labels, x, delta * 1.5)
    else:
        return None


def distributionalOracleOneVsAll(distribution, models, x, y, alpha, target=False):
    num_models = len(models)

    labels_values = []
    for labels in product(range(10), repeat=num_models):  # iterate over all possible regions
        if target:
            is_misclassified = (np.array(labels) == target).astype(np.float32)
        else:
            is_misclassified = (np.array(labels) != y).astype(np.float32)
        value = np.dot(is_misclassified, distribution)
        labels_values.append((labels, value))

    values = sorted(set([value for label, value in labels_values]), reverse=True)

    for curr_value in values:
        feasible_candidates = []
        for labels in [labels for labels, val in labels_values if val == curr_value]:
            v = tryRegionOneVsAll(models, labels, x)
            if v is not None:
                norm = np.linalg.norm(v)
                if norm <= alpha:
                    feasible_candidates.append((v, norm))
        # amongst those with the max value, return the one with the minimum norm
        if feasible_candidates:
            # break out of the loop since we have already found the optimal answer
            return min(feasible_candidates, key=lambda x: x[1])[0]
    return np.zeros(x.shape[0])  # we can't trick anything


def coordinateAscentMulti(distribution, models, x, y, alpha, target=False, greedy=False):
    # targeted needs to be the target label
    num_models = len(models)

    sol = np.zeros(x.shape)

    labels = [y] * num_models  # initialize to the original point, of length feasible_models

    model_options = dict(zip(range(num_models), distribution))
    for i in xrange(num_models):
        if greedy:  # select model with the highest probability weight
            coord = max(model_options, key=model_options.get)
            if target:
                labels[coord] = target
            else:
                labels[coord] = greedy[y]
        else:  # random: select a random model
            coord = np.random.choice(model_options.keys())
            if target:
                labels[coord] = target
            else:
                label_options = range(10)
                del label_options[y]
                labels[coord] = np.random.choice(label_options)

        del model_options[coord]

        v = tryRegionOneVsAll(models, labels, x)
        valid_sol = False
        if v is not None:
            norm = np.linalg.norm(v)
            if norm <= alpha:
                valid_sol = True
                sol = v
        if not valid_sol:
            break
    return sol


def gradientDescentTargeted(distribution, models, x, target, alpha, learning_rate=.001, T=3000, early_stop=5):
    v = np.zeros(len(x))
    best_sol = (sys.maxint, v)
    loss_queue = []
    for i in xrange(T):
        gradient = sum([-1 * p * model.gradient(np.array([x + v]), [target]) for p, model in zip(distribution, models)])[0]

        v += learning_rate * gradient
        norm = np.linalg.norm(v)
        if norm >= alpha:
            v = v / norm * alpha

        loss = np.dot(distribution, [model.rhinge_loss([x + v], [target])[0] for model in models])

        loss_queue = [loss] + loss_queue
        if i >= early_stop:
            del loss_queue[-1]
            val = loss_queue[-1]
            if sum([val == q_val for q_val in loss_queue]) == early_stop:
                break

        if loss < best_sol[0]:
            best_sol = (loss, v)

        if loss == 0:
            break
    return best_sol


def gradientDescentMulti(distribution, models, x, y, alpha, target=False):

    if target:
        return gradientDescentTargeted(distribution, models, x, target, alpha)[1]
    else:
        targets = range(10)
        del targets[y]
        noise_options = []
        for target in targets:
            sol = gradientDescentTargeted(distribution, models, x, target, alpha)
            noise_options.append(sol)
            if sol[0] == 0:
                return sol[1]
        return min(noise_options, key=lambda x: x[0])[1]


def gradientDescentNonConvex(distribution, models, x, y, alpha, learning_rate=.001, T=3000, early_stop=5):
    v = np.zeros(len(x))
    best_sol = (sys.maxint, v)
    loss_queue = []
    for i in xrange(T):
        gradient = sum([-1 * p * model.gradient_untargeted(np.array([x + v]), [y])
                        for p, model in zip(distribution, models)])[0]
        v += learning_rate * gradient
        norm = np.linalg.norm(v)
        if norm >= alpha:
            v = v / norm * alpha

        loss = np.dot(distribution, [model.untargeted_loss(np.array([x + v]), [y])[0] for model in models])

        loss_queue = [loss] + loss_queue
        if i >= early_stop:
            del loss_queue[-1]
            val = loss_queue[-1]
            if sum([val == q_val for q_val in loss_queue]) == early_stop:
                break

        if loss < best_sol[0]:
            best_sol = (loss, v)

        if loss == 0:
            break

    return best_sol[1]

greedyCoordinateAscentMulti = partial(coordinateAscentMulti, greedy=GREEDY_DICT)
randomCoordinateAscentMulti = partial(coordinateAscentMulti, greedy=False)

FUNCTION_DICT_MULTI = {"randomAscent": randomCoordinateAscentMulti,
                       "greedyAscent": greedyCoordinateAscentMulti,
                       "oracle": distributionalOracleOneVsAll,
                       "gradientDescent": gradientDescentMulti,
                       "gradientNonConvex": gradientDescentNonConvex}

